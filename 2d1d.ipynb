{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from dataset_lmdb import *\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0.dev20181014'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "th.manual_seed(int(hashlib.sha1(b'lucent').hexdigest(), 16) % (10 ** 8))\n",
    "th.cuda.manual_seed_all(int(hashlib.sha1(b'lucent').hexdigest(), 16) % (10 ** 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions / Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(modules, initializer):\n",
    "    for m in modules:\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            initializer(tensor=m.weight.data)\n",
    "            try: m.bias.data.zero_()\n",
    "            except: pass\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            initializer(tensor=m.weight.data)\n",
    "            try: m.bias.data.zero_()\n",
    "            except: pass\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            try: m.bias.data.zero_()\n",
    "            except: pass\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            m.weight.data.fill_(1)\n",
    "            try: m.bias.data.zero_()\n",
    "            except: pass\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            initializer(tensor=m.weight.data)\n",
    "            try: m.bias.data.zero_()\n",
    "            except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseConv1d(nn.Module):\n",
    "    \"Conv-BN-Act-Pool.\"\n",
    "    def __init__(self, input_size, output_size, activation, kernel_size, stride, padding, initializer, pool=True):\n",
    "        super(DenseConv1d, self).__init__()\n",
    "        self.conv = nn.Conv1d(input_size, output_size, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm1d(output_size)\n",
    "        self.act = activation(inplace=True)\n",
    "        if pool: self.pool = nn.MaxPool1d(kernel_size=(kernel_size - kernel_size % 2),\n",
    "                                          stride=(kernel_size - kernel_size % 2))\n",
    "        else: self.pool = None\n",
    "        initialize_weights(self.modules(), initializer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.act(out)\n",
    "        return self.pool(out) if self.pool else out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseConv2d(nn.Module):\n",
    "    \"Conv-BN-Act-Pool.\"\n",
    "    def __init__(self, input_size, output_size, activation, kernel_size, stride, padding, initializer, pool=True):\n",
    "        super(DenseConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_size, output_size, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(output_size)\n",
    "        self.act = activation(inplace=True)\n",
    "        if pool: self.pool = nn.MaxPool2d(kernel_size=(kernel_size - kernel_size % 2),\n",
    "                                          stride=(kernel_size - kernel_size % 2))\n",
    "        else: self.pool = None\n",
    "        initialize_weights(self.modules(), initializer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.act(out)\n",
    "        return self.pool(out) if self.pool else out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D + 1D Convolution Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubNet1D(nn.Module):\n",
    "    num_layers = 5\n",
    "\n",
    "    def __init__(self, in_dim=128, out_dim=128, activation='ReLU', initialization='kaiming_uniform'):\n",
    "        super(SubNet1D, self).__init__()\n",
    "        self.activation = getattr(nn, activation)\n",
    "        self.initialization = getattr(init, initialization)\n",
    "\n",
    "        \"1D CNN\"\n",
    "        self.conv0 = DenseConv1d(in_dim, 32, activation=self.activation, kernel_size=4, stride=1, padding=2,\n",
    "                                 initializer=self.initialization)\n",
    "        self.conv1 = DenseConv1d(160, 32, activation=self.activation, kernel_size=4, stride=1, padding=2,\n",
    "                                 initializer=self.initialization)\n",
    "        self.conv2 = DenseConv1d(192, 32, activation=self.activation, kernel_size=4, stride=1, padding=2,\n",
    "                                 initializer=self.initialization)\n",
    "        self.conv3 = DenseConv1d(224, 32, activation=self.activation, kernel_size=3, stride=1, padding=1,\n",
    "                                 initializer=self.initialization)\n",
    "        self.conv4 = DenseConv1d(256, out_dim, activation=self.activation, kernel_size=1, stride=1, padding=0,\n",
    "                                 pool=False, initializer=self.initialization)\n",
    "\n",
    "        initialize_weights(self.modules(), self.initialization)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.transpose(1, 2)\n",
    "\n",
    "        outs = [out]\n",
    "        pools = [4, 4, 4, 2, 1]\n",
    "        for i in range(SubNet1D.num_layers):\n",
    "            out = getattr(self, f'conv{i}')(torch.cat(outs, 1))\n",
    "            if pools[i] > 1:\n",
    "                outs = [F.max_pool1d(o, kernel_size=pools[i], stride=pools[i]) for o in outs]\n",
    "            outs.append(out)\n",
    "\n",
    "        \"Global Pooling\"\n",
    "        mean = F.adaptive_avg_pool1d(out, 1).squeeze(-1)\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubNet2D(nn.Module):\n",
    "    num_layers = 5\n",
    "\n",
    "    def __init__(self, in_dim=1, out_dim=128, activation='ReLU', initialization='kaiming_uniform'):\n",
    "        super(SubNet2D, self).__init__()\n",
    "        self.activation = getattr(nn, activation)\n",
    "        self.initialization = getattr(init, initialization)\n",
    "\n",
    "        \"2D CNN\"\n",
    "        self.conv0 = DenseConv2d(1, 32, activation=self.activation, kernel_size=4, stride=1, padding=2,\n",
    "                                 initializer=self.initialization)\n",
    "        self.conv1 = DenseConv2d(33, 32, activation=self.activation, kernel_size=4, stride=1, padding=2,\n",
    "                                 initializer=self.initialization)\n",
    "        self.conv2 = DenseConv2d(65, 32, activation=self.activation, kernel_size=4, stride=1, padding=2,\n",
    "                                 initializer=self.initialization)\n",
    "        self.conv3 = DenseConv2d(97, 32, activation=self.activation, kernel_size=3, stride=1, padding=1,\n",
    "                                 initializer=self.initialization)\n",
    "        self.conv4 = DenseConv2d(129, out_dim, activation=self.activation, kernel_size=1, stride=1, padding=0,\n",
    "                                 pool=False, initializer=self.initialization)\n",
    "\n",
    "        initialize_weights(self.modules(), self.initialization)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = th.unsqueeze(x.transpose(1, 2), 1)\n",
    "\n",
    "        outs = [out]\n",
    "        pools = [4, 4, 4, 2, 1]\n",
    "        for i in range(SubNet2D.num_layers):\n",
    "            out = getattr(self, f'conv{i}')(torch.cat(outs, 1))\n",
    "            if pools[i] > 1:\n",
    "                outs = [F.max_pool2d(o, kernel_size=pools[i], stride=pools[i]) for o in outs]\n",
    "            outs.append(out)\n",
    "\n",
    "        \"Global Pooling\"\n",
    "        mean = F.adaptive_avg_pool2d(out, 1).squeeze(-1).squeeze(-1)\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleCNN(nn.Module):\n",
    "    def __init__(self, in_dim=128, out_dim=128, activation='ReLU', initialization='kaiming_uniform'):\n",
    "        super(MultiScaleCNN, self).__init__()\n",
    "        self.activation = getattr(nn, activation)\n",
    "        self.initialization = getattr(init, initialization)\n",
    "\n",
    "        \"1D + 2D CNN\"\n",
    "        self.net1 = SubNet1D(in_dim, out_dim, activation, initialization)\n",
    "        self.net2 = SubNet2D(1, out_dim, activation, initialization)\n",
    "        self.regression = nn.Linear(out_dim * 2, out_dim, bias=False)\n",
    "\n",
    "        initialize_weights(self.modules(), self.initialization)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.net1(x)\n",
    "        b = self.net2(x)\n",
    "        out = self.regression(torch.cat([a, b], 1))\n",
    "        return F.relu(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataset, return_outputs=False):\n",
    "    torch.cuda.empty_cache()\n",
    "    model = model.eval()\n",
    "    avg_loss, rets = [], []\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    for batch in dataset:\n",
    "        xs, y = batch['x'], batch['y']\n",
    "        if len(xs) == 0 or len(xs[0]) == 0:\n",
    "            if return_outputs: rets.append(np.zeros(y.shape))\n",
    "            continue\n",
    "        outputs = Variable(torch.zeros(*y.shape), requires_grad=False).type(torch.cuda.FloatTensor)\n",
    "        y = Variable(y, requires_grad=False).type(torch.cuda.FloatTensor)\n",
    "        for x in xs:\n",
    "            x = Variable(x, requires_grad=False).type(torch.cuda.FloatTensor)\n",
    "            outs = model(x)\n",
    "            outputs += outs\n",
    "        outputs /= len(xs)\n",
    "        if return_outputs: rets.append(outputs.cpu().detach().numpy())\n",
    "        loss = criterion(outputs, y)\n",
    "        avg_loss.append(loss.data.item())\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if return_outputs:\n",
    "        rets = np.concatenate(rets, axis=0)\n",
    "        return np.mean(avg_loss), rets\n",
    "    return np.mean(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucent/.pyenv/versions/3.6.4/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  after removing the cwd from sys.path.\n",
      "/home/lucent/.pyenv/versions/3.6.4/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  \n",
      "/home/lucent/.pyenv/versions/3.6.4/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   1/100] Train Loss: 0.9455, Valid Loss: 0.9305.\n",
      "[Epoch   2/100] Train Loss: 0.9274, Valid Loss: 0.9247.\n",
      "[Epoch   3/100] Train Loss: 0.9213, Valid Loss: 0.9224.\n",
      "[Epoch   4/100] Train Loss: 0.9161, Valid Loss: 0.9194.\n",
      "[Epoch   5/100] Train Loss: 0.9125, Valid Loss: 0.9125.\n",
      "[Epoch   6/100] Train Loss: 0.9099, Valid Loss: 0.9187.\n",
      "[Epoch   7/100] Train Loss: 0.9079, Valid Loss: 0.9137.\n",
      "[Epoch   8/100] Train Loss: 0.9058, Valid Loss: 0.9075.\n",
      "[Epoch   9/100] Train Loss: 0.9042, Valid Loss: 0.9045.\n",
      "[Epoch  10/100] Train Loss: 0.9033, Valid Loss: 0.9054.\n",
      "[Epoch  11/100] Train Loss: 0.9025, Valid Loss: 0.9033.\n",
      "[Epoch  12/100] Train Loss: 0.9015, Valid Loss: 0.9050.\n",
      "[Epoch  13/100] Train Loss: 0.9009, Valid Loss: 0.9027.\n",
      "[Epoch  14/100] Train Loss: 0.8999, Valid Loss: 0.9034.\n",
      "[Epoch  15/100] Train Loss: 0.8994, Valid Loss: 0.9055.\n",
      "[Epoch  16/100] Train Loss: 0.8979, Valid Loss: 0.9053.\n",
      "[Epoch  17/100] Train Loss: 0.8973, Valid Loss: 0.9028.\n",
      "Epoch    16: reducing learning rate of group 0 to 2.0000e-03.\n",
      "[Epoch  18/100] Train Loss: 0.8897, Valid Loss: 0.8940.\n",
      "[Epoch  19/100] Train Loss: 0.8881, Valid Loss: 0.8964.\n",
      "[Epoch  20/100] Train Loss: 0.8871, Valid Loss: 0.8945.\n",
      "[Epoch  21/100] Train Loss: 0.8866, Valid Loss: 0.8936.\n",
      "[Epoch  22/100] Train Loss: 0.8861, Valid Loss: 0.8936.\n",
      "[Epoch  23/100] Train Loss: 0.8855, Valid Loss: 0.8940.\n",
      "[Epoch  24/100] Train Loss: 0.8852, Valid Loss: 0.8929.\n",
      "[Epoch  25/100] Train Loss: 0.8848, Valid Loss: 0.8961.\n",
      "[Epoch  26/100] Train Loss: 0.8842, Valid Loss: 0.8938.\n",
      "[Epoch  27/100] Train Loss: 0.8840, Valid Loss: 0.8940.\n",
      "[Epoch  28/100] Train Loss: 0.8833, Valid Loss: 0.8928.\n",
      "Epoch    27: reducing learning rate of group 0 to 4.0000e-04.\n",
      "[Epoch  29/100] Train Loss: 0.8808, Valid Loss: 0.8918.\n",
      "[Epoch  30/100] Train Loss: 0.8804, Valid Loss: 0.8928.\n",
      "[Epoch  31/100] Train Loss: 0.8805, Valid Loss: 0.8910.\n",
      "[Epoch  32/100] Train Loss: 0.8801, Valid Loss: 0.8928.\n",
      "[Epoch  33/100] Train Loss: 0.8799, Valid Loss: 0.8912.\n",
      "[Epoch  34/100] Train Loss: 0.8795, Valid Loss: 0.8910.\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_dataset, valid_dataset, optimizer, max_epoch, early_stop, scheduler):\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(max_epoch):\n",
    "        model = model.train()\n",
    "        avg_loss = []\n",
    "        for batch in train_dataset:\n",
    "            xs, y = batch['x'], batch['y']\n",
    "            optimizer.zero_grad(); model.zero_grad()\n",
    "            y = Variable(y).type(torch.cuda.FloatTensor)\n",
    "            outputs = Variable(torch.zeros(*y.shape)).type(torch.cuda.FloatTensor)\n",
    "            for x in xs:\n",
    "                x = Variable(x).type(torch.cuda.FloatTensor)\n",
    "                outs = model(x)\n",
    "                outputs += outs\n",
    "            outputs /= len(xs)\n",
    "            loss = criterion(outputs, y)\n",
    "            avg_loss.append(loss.data.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        tr_loss = np.mean(avg_loss)\n",
    "        val_loss = eval(model, valid_dataset)\n",
    "        print(f'[Epoch {epoch+1:3d}/{max_epoch}] Train Loss: ' +\\\n",
    "              f'{tr_loss:.4f}, Valid Loss: {val_loss:.4f}.')\n",
    "\n",
    "        if scheduler: scheduler.step(val_loss)\n",
    "        if optimizer.param_groups[0]['lr'] <= early_stop:\n",
    "            print('Early Stopping!')\n",
    "            break\n",
    "\n",
    "    ckpt_name = f'model.bin'\n",
    "    torch.save(f=ckpt_name, obj=model.state_dict())\n",
    "    return model\n",
    "\n",
    "\n",
    "\"Create a model, train the model, and finally test the model.\"\n",
    "model = MultiScaleCNN(activation='ReLU', initialization='kaiming_uniform', out_dim=100).cuda()\n",
    "max_epoch, lr, early_stop = 100, 0.01, 3.2e-6\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, eps=1e-6, weight_decay=1e-6)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                      cooldown=0, factor=0.2, patience=3, verbose=True)\n",
    "\n",
    "data = get_msd_songs(window_size=600, batch_size=20)\n",
    "model = train(model, data.train, data.valid, optimizer, max_epoch,\n",
    "              early_stop, scheduler)\n",
    "\n",
    "\"Create embeddings for every song, and then save it.\"\n",
    "t_data = get_msd_songs_all(window_size=600, batch_size=1)\n",
    "total_loss, encodings = eval(model, t_data, return_outputs=True)\n",
    "emb_path = './kor_embedding.npy'\n",
    "np.save(emb_path, encodings)\n",
    "print(f'Total Loss: {total_loss}, embedding is saved to {emb_path}.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
